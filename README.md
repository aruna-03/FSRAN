# FSRAN
Medical image classifiers based on deep learning have shown impressive results across a range of diagnostic applications. However, their clinical usefulness is hampered and their trustworthiness is undermined by their susceptibility to adversarial attacks, which exploit vulnerabilities in the models and may lead to incorrect diagnoses. Traditional adversarial attack methods, such as the Fast Gradient Sign Method (FGSM) and Carlini and Wagner Attack, have highlighted these vulnerabilities by crafting imperceptible perturbations to deceive the classifiers. Despite their effectiveness in exposing weaknesses in deep learning models, traditional attack methods suffer from several disadvantages, including lack of interpretability, limited transferability, sensitivity to perturbation magnitude, vulnerability to defense mechanisms, computational intensity, and ethical concerns. In this article, we propose a unique approach to address this challenge. We introduce Feature Space Restricted Adversarial Networks (FSRANs) to produce lesion-specific perturbations using Targeted Gradients with Random Sampling (TGRS) in medical pictures. Our strategy combines random sampling with targeted gradients to craft perturbations that fool classifiers while preserving the therapeutic relevance of the images. We apply our methods to two different datasets: chest x-ray and dermoscopy, allowing for customized lesion-specific perturbations tailored for each modality. Through comprehensive testing, we assess the efficacy of FSRANs in enhancing the resilience of deep learning models against hostile attacks. Our findings illuminate the potential of FSRANs to mitigate the impact of traditional adversarial attacks and bolster the robustness of medical image classifiers in clinical settings.
